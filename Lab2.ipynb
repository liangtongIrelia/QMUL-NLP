{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by implementing the parseReview and the preProcess functions. Given a line of a tab-separated text file, parseReview should return a triple containing the identifier of the review (as an integer), the review text itself, and the label (either ‘fake’ or ‘real’). The preProcess function should turn a review text (a string) into a list of tokens.\n",
    "Hint: you can start by tokenising on white space; but you might want to think about some simple normalisation too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The next step is to implement the toFeatureVector function. Given a preprocessed review (that is, a list of tokens), it will return a Python dictionary that has as its keys the tokens, and as values the weight of those tokens in the preprocessed reviews. The weight could be simply the number of occurrences of a token in the preprocessed review, or it could give more weight to specific words. While building up this feature vector, you may want to incrementally build up a global featureDict, which should be a list or dictionary that keeps track of all the tokens in the whole review dataset. While a global feature dictionary is not strictly required for this coursework, it will help you understand which features (and how many!) you are using to train your classifier and can help understand possible performance issues you encounter on the way.\n",
    "Hint: start by using binary feature values; 1 if the feature is present, 0 if it’s not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the loadData function already present in the template file, you are now ready to process the review data from amazon_reviews.txt. In order to train a good classifier, finish the implementation of the crossValidate function to do a 10-fold cross validation on the training data. Make use of the given functions trainClassifier and predictLabels to do the cross-validation. Make sure that your program stores the (average) precision, recall, f1 score, and accuracy of your classifier in a variable cv_results.\n",
    "Hint: the package sklearn.metrics contains many utilities for evaluation metrics - you could try precision recall fscore support to start with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that you have the numbers for accuracy of your classifier, think of ways to improve this score. Things to consider:\n",
    "• Improve the preprocessing. Which tokens might you want to throw out or preserve?\n",
    "\n",
    "• What about punctuation? Do not forget normalisation and lemmatising - what aspects of this might be useful?\n",
    "\n",
    "• Think about the features: what could you use other than unigram tokens from the review texts? It may be useful to look beyond single words to combinations of words or characters. Also the feature weighting scheme: what could you do other than using binary values?\n",
    "\n",
    "• You could consider playing with the parameters of the SVM (cost parameter? per-class weighting?)\n",
    "\n",
    "Report what methods you tried and what the effect was on the classifier performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab2_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now look beyond textual features of the review. The data set contains a number of other features for each review (rating, verified purchase, product category, product ID, product title, review title). How can the inclusion of these features improve your classifier’s performance? Pick three of these metadata types to use as additional features and report how they improve the classifier performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWithout any operation:\\nCurrent average accuracy is 0.618154761904762\\nCurrent average precision is 0.6185732469315799\\nCurrent average recall is 0.618154761904762\\nCurrent average fscore is 0.6181181376909236\\n\\nFor question 4:\\nAfter the operation include:using the stop words,removing punctuations,converting all letters to lower case,Using standart stemmer from the nltk\\nsynonym replacement,and change the parameters of the SVM\\nCurrent average accuracy is 0.642202380952381\\nCurrent average precision is 0.6432759799590053\\nCurrent average recall is 0.642202380952381\\nCurrent average fscore is 0.6416551643326958\\nThis seems to have some improvement, but the effect is not obvious\\n\\nFor question 5:\\nI choose \"Rating\",\"VERIFIED_PURCHASE\",\"REVIEW_TITLE\"\\nCurrent average accuracy is 0.8036309523809523\\nCurrent average precision is 0.8085790394518841\\nCurrent average recall is 0.8036309523809523\\nCurrent average fscore is 0.8028697389613784\\nThis seems to have a big improvement on classifier, so I think the better way to improve the performance of classifier is to add more features\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Without any operation:\n",
    "Current average accuracy is 0.618154761904762\n",
    "Current average precision is 0.6185732469315799\n",
    "Current average recall is 0.618154761904762\n",
    "Current average fscore is 0.6181181376909236\n",
    "\n",
    "For question 4:\n",
    "After the operation include:using the stop words,removing punctuations,converting all letters to lower case,Using standart stemmer from the nltk\n",
    "synonym replacement,and change the parameters of the SVM\n",
    "Current average accuracy is 0.642202380952381\n",
    "Current average precision is 0.6432759799590053\n",
    "Current average recall is 0.642202380952381\n",
    "Current average fscore is 0.6416551643326958\n",
    "This seems to have some improvement, but the effect is not obvious\n",
    "\n",
    "For question 5:\n",
    "I choose \"Rating\",\"VERIFIED_PURCHASE\",\"REVIEW_TITLE\"\n",
    "Current average accuracy is 0.8036309523809523\n",
    "Current average precision is 0.8085790394518841\n",
    "Current average recall is 0.8036309523809523\n",
    "Current average fscore is 0.8028697389613784\n",
    "This seems to have a big improvement on classifier, so I think the better way to improve the performance of classifier is to add more features\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#path = \"/Users/irelia/Desktop/jupyter-notebook/NLP/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from a file and append it to the rawData\n",
    "def loadData(path, Text=None):\n",
    "    with open(path) as f:\n",
    "        reader = csv.reader(f, delimiter='\\t')\n",
    "        for line in reader:\n",
    "            (ID, Text, Label,Rating,VERIFIED_PURCHASE,REVIEW_TITLE) = parseReview(line)\n",
    "            rawData.append((ID, Text, Label,Rating,VERIFIED_PURCHASE,REVIEW_TITLE))\n",
    "            preprocessedData.append((ID, preProcess(Text), Label,Rating,VERIFIED_PURCHASE,preProcess(REVIEW_TITLE)))\n",
    "        del preprocessedData[0]\n",
    "        del rawData[0]\n",
    "        \n",
    "def splitData(percentage):\n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (_, Text, Label,Rating,VERIFIED_PURCHASE,REVIEW_TITLE) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]:\n",
    "        w = Text+' '+Rating+' '+VERIFIED_PURCHASE+' '+REVIEW_TITLE\n",
    "        trainData.append((toFeatureVector(preProcess(w)),Label))\n",
    "    for (_, Text, Label,Rating,VERIFIED_PURCHASE,REVIEW_TITLE) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]:\n",
    "        w = Text+' '+Rating+' '+VERIFIED_PURCHASE+' '+REVIEW_TITLE\n",
    "        testData.append((toFeatureVector(preProcess(w)),Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 1\n",
    "\n",
    "# Convert line from input file into an id/text/label tuple\n",
    "def parseReview(reviewLine):\n",
    "    # Should return a triple of an integer, a string containing the review, and a string indicating the label\n",
    "    #Id,Text,Label\n",
    "    ID,Text,Label,Rating,VERIFIED_PURCHASE,REVIEW_TITLE = reviewLine[0],reviewLine[8],reviewLine[1],reviewLine[2],reviewLine[3],reviewLine[7]\n",
    "    return (ID, Text, Label,Rating,VERIFIED_PURCHASE,REVIEW_TITLE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
    "\n",
    "# Input: a string of one review\n",
    "stop_words = set(stopwords.words('english'))\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preProcess(text):\n",
    "    # Should return a list of tokens\n",
    "    text = word_tokenize(text)\n",
    "    text_preprocessed = []\n",
    "    for word in text:\n",
    "        if word.isalpha(): # removing punctuation\n",
    "            if word not in stop_words: # removing stopwords or \"too common\" words\n",
    "                word = word.lower() # converting all letters to lower case\n",
    "                word = wordnet_lemmatizer.lemmatize(word) \n",
    "                word = stemmer.stem(word) # Using standart stemmer from the nltk\n",
    "                text_preprocessed.append(word)\n",
    "    return text_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 2\n",
    "featureDict = {} # A global dictionary of features\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "def toFeatureVector(tokens):\n",
    "    # Should return a dictionary containing features as keys, and weights as values\n",
    "    featureVector = {}\n",
    "    for token in tokens:\n",
    "        if token not in featureVector:\n",
    "            featureVector[token] = 1.0\n",
    "        else:\n",
    "            featureVector[token] = float(featureVector[token] + 1)\n",
    "\n",
    "        if token not in featureDict:\n",
    "            featureDict[token] = 1.0\n",
    "        else:\n",
    "            featureDict[token] = float(featureDict[token] + 1)\n",
    "    return featureVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING AND VALIDATING OUR CLASSIFIER\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC(C = 0.001, class_weight = \"balanced\"))])\n",
    "    return SklearnClassifier(pipeline).train(trainData)#return a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 3\n",
    "'''\n",
    "p = precision_score(y_true, y_pred, average='binary')\n",
    "r = recall_score(y_true, y_pred, average='binary')\n",
    "f1score = f1_score(y_true, y_pred, average='binary')\n",
    "accuracy_score(y_true, y_pred)\n",
    "'''\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)\n",
    "    cv_results = []\n",
    "    foldSize = int(len(dataset)/folds)\n",
    "    #print(len(dataset))\n",
    "    for i in range(0,len(dataset),foldSize):\n",
    "    # Replace by code that trains and tests on the 10 folds of data in the dataset\n",
    "        print(\"fold start %d foldSize %d\" %(i,foldSize))\n",
    "        data_Test = dataset[i:i+foldSize]\n",
    "        data_Train = dataset[:i]+dataset[i+foldSize:]\n",
    "        classifier = trainClassifier(data_Train)\n",
    "        y_true = list(map(lambda x : x[1],data_Test))\n",
    "        y_pred = classifier.classify_many(map(lambda x : x[0],data_Test))\n",
    "        cv_results.append(accuracy_score(y_true,y_pred))\n",
    "        cv_results.append(precision_score(y_true,y_pred,average = 'weighted'))\n",
    "        cv_results.append(recall_score(y_true,y_pred,average = 'weighted'))\n",
    "        cv_results.append(f1_score(y_true,y_pred,average = 'weighted'))\n",
    "    return cv_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTING LABELS GIVEN A CLASSIFIER\n",
    "\n",
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: toFeatureVector(preProcess(t[1])), reviewSamples))\n",
    "\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(reviewSample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 21000 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "Now 21000 rawData, 16800 trainData, 4200 testData\n",
      "Training Samples: \n",
      "16800\n",
      "Features: \n",
      "21570\n"
     ]
    }
   ],
   "source": [
    "# MAIN\n",
    "\n",
    "# loading reviews\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "preprocessedData = [] # the preprocessed reviews (just to see how your preprocessing is doing)\n",
    "trainData = []        # the training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "# the output classes\n",
    "fakeLabel = 'fake'\n",
    "realLabel = 'real'\n",
    "# references to the data files\n",
    "reviewPath = \"amazon_reviews.txt\"\n",
    "\n",
    "## Do the actual stuff\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath) \n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData(0.8)\n",
    "#print(trainData)\n",
    "# We print the number of training samples and the number of features\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold start 0 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 1680 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 3360 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 5040 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 6720 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 8400 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 10080 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 11760 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 13440 foldSize 1680\n",
      "Training Classifier...\n",
      "fold start 15120 foldSize 1680\n",
      "Training Classifier...\n",
      "[0.8        0.80544467 0.8        0.79924925 0.81190476 0.81630518\n",
      " 0.81190476 0.8114304  0.81488095 0.81941665 0.81488095 0.81428832\n",
      " 0.79047619 0.79428571 0.79047619 0.78965774 0.80714286 0.81361231\n",
      " 0.80714286 0.80675986 0.80297619 0.80650833 0.80297619 0.80225241\n",
      " 0.80535714 0.81042821 0.80535714 0.80403581 0.80595238 0.81030051\n",
      " 0.80595238 0.80498811 0.7875     0.79224787 0.7875     0.78676253\n",
      " 0.81130952 0.81655725 0.81130952 0.81046756]\n"
     ]
    }
   ],
   "source": [
    "cv_results = crossValidate(trainData,10)\n",
    "cv_results = np.asarray(cv_results)\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy,precision,recall and fscore is \n",
      " [[0.8        0.80544467 0.8        0.79924925]\n",
      " [0.81190476 0.81630518 0.81190476 0.8114304 ]\n",
      " [0.81488095 0.81941665 0.81488095 0.81428832]\n",
      " [0.79047619 0.79428571 0.79047619 0.78965774]\n",
      " [0.80714286 0.81361231 0.80714286 0.80675986]\n",
      " [0.80297619 0.80650833 0.80297619 0.80225241]\n",
      " [0.80535714 0.81042821 0.80535714 0.80403581]\n",
      " [0.80595238 0.81030051 0.80595238 0.80498811]\n",
      " [0.7875     0.79224787 0.7875     0.78676253]\n",
      " [0.81130952 0.81655725 0.81130952 0.81046756]]\n"
     ]
    }
   ],
   "source": [
    "cv_results = cv_results.reshape(10,4)\n",
    "print(\"accuracy,precision,recall and fscore is\",'\\n',cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current average accuracy is 0.80375\n",
      "Current average precision is 0.8085106702118777\n",
      "Current average recall is 0.80375\n",
      "Current average fscore is 0.802989199381378\n"
     ]
    }
   ],
   "source": [
    "print(\"Current average accuracy is \" + str(np.mean(cv_results[:,0], axis=0)))\n",
    "print(\"Current average precision is \" + str(np.mean(cv_results[:,1], axis=0)))\n",
    "print(\"Current average recall is \" + str(np.mean(cv_results[:,2], axis=0)))\n",
    "print(\"Current average fscore is \" + str(np.mean(cv_results[:,3], axis=0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
